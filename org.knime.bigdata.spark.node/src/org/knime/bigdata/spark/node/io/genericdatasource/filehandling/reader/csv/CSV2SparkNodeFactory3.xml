<?xml version="1.0" encoding="UTF-8"?>
<knimeNode icon="./icon.png"
		type="Source"
		xmlns="http://knime.org/node/v4.1"
		xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
		xsi:schemaLocation="http://knime.org/node/v4.1 http://knime.org/node/v4.1.xsd">

	<name>CSV to Spark</name>
	<shortDescription>Creates a Spark DataFrame/RDD from given CSV file.</shortDescription>
	<fullDescription>
		<intro>
            <p><i>This node supports the 
            </i><a href="https://docs.knime.com/latest/analytics_platform_file_handling_guide/index.html#path"><i>path flow variable.</i></a>
            <i> For further information about file handling in general see the 
            </i><a href="https://docs.knime.com/latest/analytics_platform_file_handling_guide/index.html"><i>File Handling Guide.</i></a><br/></p>
			Creates a Spark DataFrame/RDD from given CSV file.
			
			See <a href="https://github.com/databricks/spark-csv">CSV Data Source documentation</a> for more information.
			
			<p><b>Notice:</b> This feature requires at least Apache Spark 1.5.</p>
		</intro>
		
		<tab name="Settings">
			<option name="Mode">
	            Select if you like to read a file or folder.
			</option>
	
			<option name="File/Folder">
				Enter the input path. The required syntax of a path depends on the connected file system. The node
	            description of the respective connector node describes the required path format.
				You can also choose a previously selected file from the drop-down list, or select a location
				from the &quot;Browse...&quot; dialog. Note that browsing is disabled in some cases:
				<ul>
					<li>
					Browsing is disabled if the connector node hasn't been executed since the workflow has been opened.
					(Re)execute the connector node to enable browsing.</li>
				</ul>
                <i>The location can be exposed as or automatically set via a 
                </i><a href="https://docs.knime.com/latest/analytics_platform_file_handling_guide/index.html#path">
                <i>path flow variable.</i></a>
			</option>
	        <option name="Driver">Upload data source driver or depend on cluster side provided driver.</option>
       		<option name="Header">First line of files will be used to name columns and will not be included in data.</option>
			<option name="Delimiter">Character used as delimiter between columns (supports <a href="http://docs.oracle.com/javase/specs/jls/se8/html/jls-3.html#jls-3.10.6">escape sequences</a>, e.g. <i>\t</i> or <i>\u0123</i>).</option>
			<option name="Quote character">Quote character (delimiters inside quotes are ignored).</option>
			<option name="Escape character">Escape character (escaped quote characters are ignored).</option>
			<option name="Mode">
				Determines the parsing mode. By default it is PERMISSIVE.
				<ul>
					<li><b>PERMISSIVE:</b> tries to parse all lines: nulls are inserted for missing tokens and extra tokens are ignored.</li>
					<li><b>DROPMALFORMED:</b> drops lines which have fewer or more tokens than expected or tokens which do not match the schema.</li>
					<li><b>FAILFAST:</b> aborts with a RuntimeException if encounters any malformed line.</li>
				</ul>
			</option>
			<option name="Charset">Valid charset name (see <a href="https://docs.oracle.com/javase/8/docs/api/java/nio/charset/Charset.html">java.nio.charset.Charset</a>).</option>
			<option name="Schema">Automatically infers column types. It requires one extra pass over the data. All types will be assumed string otherwise.</option>
			<option name="Comments">Skip lines beginning with this character.</option>
			<option name="Null value">Specifies a string that indicates a null value, any fields matching this string will be set as nulls in the DataFrame.</option>
			<option name="Date format">
				Specifies a string that indicates the date format to use when reading dates or timestamps. Custom date formats follow the formats at <a href="https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html">java.text.SimpleDateFormat</a>. This applies to both DateType and TimestampType. By default, it is null which means trying to parse times and date by java.sql.Timestamp.valueOf() and java.sql.Date.valueOf().
			</option>
		</tab>
	</fullDescription>

	<ports>
		<inPort index="0" name="File system connection">Spark compatible connection (HDFS, WebHDFS, HttpFS, S3, Blob Storage, ...)</inPort>
		<inPort index="1" name="Spark context">Spark context</inPort>
		<outPort index="0" name="Spark data">Spark DataFrame/RDD</outPort>
	</ports>
</knimeNode>
