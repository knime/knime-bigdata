<?xml version="1.0" encoding="UTF-8"?>
<knimeNode xmlns="http://knime.org/node/v2.12" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
    xsi:schemaLocation="http://knime.org/node/v2.12 http://knime.org/node/v2.12.xsd" 
    type="Learner" icon="icon.png">
    <name>Spark Gradient Boosted Trees Learner (Regression)</name>
	<shortDescription>Random forests induction performed in Spark.</shortDescription>
	<fullDescription>
		<intro>
			Gradient Boosted Trees are ensembles of decision trees. They iteratively train decision trees in order
			to minimize a loss function. This node uses the
			<a
				href="http://spark.apache.org/docs/2.4.3/ml-classification-regression.html#gradient-boosted-trees-gbts">
				spark.ml Gradient Boosted Trees
			</a>
			implementation to train a regression model in Spark. The target column must be numerical, whereas the
			feature columns can be either nominal or numerical.
			<p>
				Use the
				<i>Spark Predictor (Regression)</i>
				node to apply the learned model to unseen data.
			</p>
			<p>
				Please refer to the
				<a href="http://spark.apache.org/docs/2.4.3/mllib-ensembles.html#gradient-boosted-trees-gbts">Spark documentation</a>
				for a full description of the underlying algorithm.
			</p>
			<p><i>This node requires at least Apache Spark 2.0.</i></p>
		</intro>
		
		<tab name="Settings">
			<option name="Target column">
				A numeric column that contains the values to train with. Rows with missing values in this column will be
				ignored during model training.
			</option>

			<option name="Feature Columns">
				The feature columns to learn the model with. Both nominal and numeric columns are supported.
				The dialog allows to select the columns manually (by moving them to the right panel) or via a
				wildcard/regex selection (all columns whose names match the wildcard/regex are used for learning).
				In case of manual selection, the behavior for new columns (i.e. that are not available at the
				time you configure the node) can be specified as either Enforce exclusion (new columns are
				excluded and therefore not used for learning) or Enforce inclusion (new columns are included
				and therefore used for learning).
			</option>

			<option name="Number of models">The number of decision tree models in the ensemble model.
				Increasing this number makes the model more expressive and improves training data accuracy. However,
				increasing it too much may lead to overfitting. Also, increasing this number directly increases the
				time required to train the ensemble, because the trees need to be trained sequentially.
			</option>
			
			<option name="Loss function">
				The loss function to minimize by the learner algorithm:
				<ul>
				  <li><i>squared (default)</i>: Squared Error, also known as L2 loss.</li>
				  <li><i>absolute</i>: Absolute Error, also known as L1 loss.</li>
				</ul> 
				More information is available in the <a href="http://spark.apache.org/docs/2.4.3/mllib-ensembles.html#losses">
				Spark documentation</a>.
			</option>

			<option name="Max tree depth">
				Maximum depth of the decision trees (>= 0).
			</option>
			
			<option name="Min rows per tree node">
				Minimum number of rows each tree node must have. If a split causes the left or right child node to have
				fewer rows, the split will be discarded as invalid. Must be >= 1.
			</option>

			<option name="Min information gain per split">
				Minimum information gain for a split to be considered.
			</option>
			
			<option name="Max number of bins">
				Number of bins to use when discretizing continuous features. Increasing the number of bins
				means that the algorithm will consider more split candidates and make more fine-grained decisions on
				how to split. However, it also increases the amount of computation and communication that needs to be performed
				and hence increases training time. Additionally, the number of bins must be at least the maximum
				number of distinct values for any nominal feature.
			</option>
		</tab>
		
		<tab name="Advanced">
			<option name="Learning rate">Learning rate in interval (0, 1] for shrinking the contribution of each decision
			tree in the ensemble. This parameter should not need to be tuned often. Decreasing this value may improve
			stability, if the algorithm behavior seems unstable.
			</option>
		
			<option name="Data sampling (rows)">
				Sampling the rows is also known as bagging, a very popular ensemble learning strategy. If sampling is
				disabled (default), then each decision tree is trained on the full data set. Otherwise each
				tree is trained with a different data sample that contains the configured fraction of rows of the
				original data. 
			</option>
			
			<option name="Feature sampling">
				Feature sampling is also called random subspace method or attribute bagging. Its most famous application
				are Random Forests, but it can also be used for Gradient Boosted Trees. This option specifies the sample size
				for each split at a tree node:
				<ul>
				  <li><i>auto</i>: if "Max number of models" is one, then this is the same as "all", otherwise "sqrt" will be used.</li>
				  <li><i>all (default)</i>: each sample contains all features.</li>
				  <li><i>sqrt</i>: sample size is sqrt(number of features)</li>
				  <li><i>log2</i>: sample size is log2(number of features)</li>
				  <li><i>onethird</i>: sample size is 1/3 of the features</li> 
				</ul> 
			</option>
			
			<option name="Use static random seed">
				Seed for generating random numbers. Randomness is used when sampling rows and features, as well as binning numeric
				features during splitting.
			</option>
		</tab>
	</fullDescription>

    <ports>
		<inPort index="0" name="Input data">Input Spark DataFrame with training data.</inPort>
		<outPort index="0" name="Feature importance measures">
			Table with estimates of the importance of each feature. The features are listed in order of decreasing
			importance and are normalized to sum up to 1.
		</outPort>
		<outPort index="1" name="Spark ML Gradient Boosted Trees model (regression)">Spark ML Gradient Boosted Trees model (regression)</outPort>
    </ports>
</knimeNode>
