<?xml version="1.0" encoding="utf-8"?>
<knimeNode xmlns="http://knime.org/node/v3.1" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xsi:schemaLocation="http://knime.org/node/v3.1 http://knime.org/node/v3.1.xsd"
    icon="icon.png" type="Manipulator">
	<name>Spark Pivot</name>
	<shortDescription>
		Pivots and groups the given Spark DataFrame/RDD by the selected columns for pivoting 
    	and	grouping. Also performs aggregations for each pivot value.
	</shortDescription>
	<fullDescription>
		<intro>
	        <p>
				Performs a pivoting on the given Spark DataFrame/RDD using a selected
				number of columns for grouping and one column for pivoting. Each combination of
				values in the grouping columns will result into an output
				row. Each combination of pivot values and aggregations becomes a
				new output column.
        	</p>

            <p>
	            The aggregations to perform can be specified (a) by selecting the columns directly in the
	            "Manual Aggregation" tab, and (b) by a column name search pattern or regular expression in the
	            "Pattern Based Aggregation" tab, and (c) by column type in the "Type Based Aggregation" tab.
	            Each input column is only considered once, i.e. columns that are added directly on the
	            "Manual Aggregation" tab are ignored even if their name matches a search pattern on the
	            "Pattern Based Aggregation" tab or their type matches a type on the
	            "Type Based Aggregation" tab. The same holds for columns that are added based on a search pattern.
	            They are ignored even if they match a criterion that has been defined in the "Type Based Aggregation" tab.
            </p>

            <p>
	            A detailed description of the available aggregation methods can be
	            found on the 'Description' tab in the node dialog. Further information can be also found on the
	            <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#aggregations">Spark documentation</a>
	             and the
	            <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$">Spark API documentation</a>.
            </p>
            <p><i>This node requires at least Apache Spark 2.0.</i></p>
		</intro>
        <tab name="Groups">
    		<option name="Group settings">
    			Select one or more column(s) according to which the group(s) is/are created.
    		</option>
        </tab>
        <tab name="Advanced settings">
            <option name="Column naming">
                The names of the resulting pivot/aggregation columns depend on the
                selected naming schema. The name of each such column consists of a
                value from the pivot column, a separator ("+") and then the name of an aggregation,
                which is chosen according to one of the following schemata:
                <ul>
                    <li>Keep original name(s):
                        Keeps the original names of the input columns. With this option, you can
                        use each input column only for one aggregation to prevent duplicate column names.
                    </li>
                    <li>Aggregation method (column name):
                        Uses the aggregation method
                        first and appends the name of the input column in brackets.
                    </li>
                    <li>Column name (aggregation method):
                        Uses the the name of the input column first and
                        appends the aggregation method in brackets.
                    </li>
                </ul>
            </option>
            <option name="Add COUNT(*)">
                Tick this option to perform the COUNT(*) aggregation in addition to the selected aggregations.
            </option>
            <option name="column name">
                The display of the COUNT(*) aggregation. Only enabled if the "Add COUNT(*)" option is selected.
            </option>
        </tab>
        <tab name="Pivot">
            <description>
				The "Pivot" tab allows to transpose values of <i>one</i> input column into	individual output columns.
				The values can be computed by Spark, or manually specified in the dialog.
				To pivot over <i>multiple</i> columns, you can use the <i>Spark SQL</i> with the <i>concat()</i>
				function before pivoting.
			</description>

			<option name="Column">
				The column containing the values to transpose.
			</option>

			<option name="Values">
				<ul>
                <li>
	                <i>Automatic:</i> If set, Spark will automatically compute and sort the pivot values. To avoid the excessive
	                creation of new columns, a <i>limit</i> can be set to only take the top-k pivot values.
	                Selecting this option implies that Spark Pivot materializes the ingoing Spark DataFrame/RDD,
	                which may take some time.
                </li>
                <li>
                	<i>Manual:</i> If set, it allows the manual definition of the pivot values. This is useful if (a) you
                	are only interested in a small subset of the pivot values, or (b) the resulting columns should be ordered in a specific way,
                	or (c) you want to avoid materialization of the ingoing DataFrame/RDD to speed up pivoting performance.
                </li>
                </ul>
			</option>
			
			<option name="Ignore missing values">
				Ignore rows containing missing values in pivot column. This options is only available when
				<i>Automatic</i> is selected above.
			</option>
        </tab>
        <!--// TODO:
        <tab name="Time Based Window">
            <option name="Enable">
                Enable row grouping by a time based window.
            </option>
            <option name="Column">
                A column with timestamp values to use.
            </option>
            <option name="Window duration">
                A string specifying the width of the window, e.g. <i>10 minutes</i>,
                <i>1 second</i>. See <i>org.apache.spark.unsafe.types.CalendarInterval</i> for
                valid duration identifiers. Note that the duration is a fixed length of
                time, and does not vary over time according to a calendar. For example,
                <i>1 day</i> always means 86,400,000 milliseconds, not a calendar day.
            </option>
            <option name="Slide duration">
                <i>Optional:</i>
                A string specifying the sliding interval of the window, e.g. <i>1 minute</i>.
                A new window will be generated every <i>slide duration</i>. Must be less than
                or equal to the <i>window duration</i>. Check
                <i>org.apache.spark.unsafe.types.CalendarInterval</i> for valid duration
                identifiers. This duration is likewise absolute, and does not vary
                according to a calendar.
            </option>
            <option name="Start time">
                <i>Optional:</i>
                The offset with respect to 1970-01-01 00:00:00 UTC with which to start
                window intervals. For example, in order to have hourly tumbling windows that
                start 15 minutes past the hour, e.g. 12:15-13:15, 13:15-14:15... provide
                <i>start time</i> as <i>15 minutes</i>.
            </option>
        </tab>
        -->
        <tab name="Manual Aggregation">
            <description>
			In the "Manual Aggregation" tab you can select one or more columns for aggregation.
            </description>

            <option name="Aggregation settings">
                Select one or more column(s) for aggregation from the available 
                columns list. Change the aggregation method in the Aggregation 
                column of the table. You can add the same column multiple
                times. In order to change the aggregation method of more than one 
                column select all columns to change, open the context menu with a 
                right mouse click and select the aggregation method to use.
            </option>
            <option name="Parameter">
                The parameter column shows an "Edit" button for all 
                aggregation operators that require additional information. 
                Clicking on the "Edit" button opens the parameter dialog 
                which allows changing the operator specific settings.
            </option>
        </tab>
        <tab name="Pattern Based Aggregation">
            <description>
	            In the "Pattern Based Aggregation" tab you can assign aggregation methods to columns based on a
	            search pattern. The pattern can be either a string with wildcards or a
	            <a href="https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html#sum">regular expression</a>.
	            Columns where the name matches the pattern but where the data type is not compatible with the
	            selected aggregation method are ignored. Only columns that have not been selected as group column or
	            that have not been selected as aggregation column on the "Manual Aggregation" tab are considered.
	        </description>

            <option name="Aggregation settings">
                Use the "Add" button to add a new row with a search pattern to the aggregation settings.
                The search pattern can either be a string with wildcards or a 
                <a href="https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html#sum">regular expression</a>.
                Supported wildcards are * (matches any number of characters) and ? (matches one character) e.g. KNI*
                would match all strings that start with KNI such as KNIME whereas KNI? would match only strings that start
                with KNI followed by a fourth character. 
                Double click the "Search pattern" cell to edit the pattern. 
                The cell is colored in red if the pattern is invalid.
            </option>
            <option name="RegEx">Tick this option if the search pattern is a regular expression otherwise
            it is treated as string with wildcards ('*' and '?').</option>
            <option name="Parameter">
                The parameter column shows an "Edit" button for all 
                aggregation operators that require additional information. 
                Clicking on the "Edit" button opens the parameter dialog 
                which allows changing the operator specific settings.
            </option>
        </tab>
        <tab name="Type Based Aggregation">
            <description>
	            The "Type Based Aggregation" tab allows to select an aggregation method for all columns of a certain
	            data type e.g. to compute the mean for all numerical columns (DoubleCell). Only columns that have not
	            been handled by the other tabs e.g. group, column based and pattern based are considered.
	            The data type list to choose from contains basic types e.g String, Double, etc. and all data types
	            the current input table contains.
            </description>

            <option name="Aggregation Settings">
                Select one or more data type from the available type list. 
                Change the aggregation method in the Aggregation 
                column of the table. You can add the same data type multiple
                times. The list contains standard types e.g. Double, String etc. and all types of the input table.
            </option>
            <option name="Parameter">
                The parameter column shows an "Edit" button for all 
                aggregation operators that require additional information. 
                Clicking on the "Edit" button opens the parameter dialog 
                which allows changing the operator specific settings.
            </option>
        </tab>
	</fullDescription>
	<ports>
		<inPort index="0" name="Spark DataFrame/RDD">
			Spark DataFrame/RDD to apply pivoting on.
		</inPort>
		<outPort index="0" name="Spark DataFrame/RDD">
			DataFrame/RDD with pivoted data.
		</outPort>
	</ports>
</knimeNode>
