<?xml version="1.0" encoding="UTF-8"?>
<knimeNode xmlns="http://knime.org/node/v2.12" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
    xsi:schemaLocation="http://knime.org/node/v2.12 http://knime.org/node/v2.12.xsd"
    type="Source" icon="../icon.png">
	<name>Spark RDD Java Snippet (Source)</name>
	   <shortDescription>Execute arbitrary java code in Spark.</shortDescription>
    <fullDescription>
        <intro>
            <p>This node allows you to execute arbitrary java code to create a Spark RDD e.g.
            by reading a file from HDFS (See provided templates).
             Simply enter the java code in the text area.</p>
            <p>Note, that this node also supports flow variables as input to your Spark job. 
            To use a flow variable simply double click on the variable in the "Flow Variable List".</p>
            <p>It is also possible to use external java libraries. In order to 
            include such external jar or zip files, add their location in the 
            "Additional Libraries" tab using the control buttons.
            For details see the "Additional Libraries" tab description below.
            <br/>
            <b>The used libraries need to be present on your cluster and added to the class path of your Spark job server.
            They are not automatically uploaded!</b>
            <br/>
            </p>
            
            <p>You can define reusable templates with the "Create templates..." 
            button. Templates are stored in the users workspace by default and can be accessed via the "Templates"
            tab. For details see the "Templates" tab description below.</p>

            <p>For Spark 2.2 and above, this node compiles the snippet code with Java 8 support, otherwise it uses Java 7.</p>
        </intro>
        <tab name="Java Snippet">
            <option name="Flow Variable List">
              The list contains the flow variables that are currently available at
              the node input. Double clicking any of the entries will insert the
              respective identifier at the current cursor position (replacing the
              selection, if any).
            </option>
            <option name="Snippet text area">
              <p>Enter your java code here.</p>

              <p>
	          The <i>JavaSparkContext</i> can be accessed via the method input parameter <i>sc</i>.</p>

              <p><i>Output Schema:</i><br/>
              The schema (e.g. data table specification) of the returned <i>JavaRDD&lt;Row&gt;</i> is by default 
              derived automatically by looking at the top 10 rows of the returned <i>JavaRDD&lt;Row&gt;</i>. 
              However you can also specify the schema programmatically by overwriting the <i>getSchema()</i> method. 
              For an example on how to implement the method have a look at the "Create result schema manually" template 
              in the "Templates" tab.</p>

              <p><b>Flow variables:</b><br/>
              You can access input flow variables by defining them in the <b>Input</b> table.
              To define a flow variable simply double click on the variable in the "Flow Variable list".</p>

              <p>You can hit <i>ctrl+space</i> to get an auto completion box with all
              available classes, methods and fields. When you select a class and hit
              <i>enter</i> a import statement will be generated if missing.</p>          

              <p>Note, that the snippet allows to define custom global variables and
              custom imports. To view the hidden editor parts simply click on the plus symbols in the editor.</p>
            </option>
            <option name="Input">
              Define system input fields for the snippet text area. Every field will 
              be populated with the data of the defined input during execution. 
            </option>
       </tab>
       <tab name="Additional Libraries">
           <description>
               Allows you to add additional jar files to the java snippet class path.
               <br/>
               <b>The used libraries need to be present on your cluster and added to the class path of your Spark job server.
            They are not automatically uploaded!</b>
           </description>
           <option name="Add File(s)">Allows you to include local jar files.</option>
           <option name="Add KNIME URL...">Allows you to add workflow relative jar files.</option>
       </tab>
       <tab name="Templates">
           <description>Provides predefined templates and allows you to define new reusable templates by saving the
           current snippet state.</description>
           <option name="Category">Groups templates into different categories.</option>
           <option name="Apply">Overwrites the current node settings with the template settings.</option>
           <option name="Java Snippet">Preview of the template code.</option>
           <option name="Additional Libraries">Preview of the additional jars.</option>
       </tab>
    </fullDescription>

    <ports>
        <inPort index="0" name="Spark context">Required Spark context.</inPort>
        <outPort index="0" name="Spark Data">The new created Spark RDD.</outPort>
    </ports>
</knimeNode>
