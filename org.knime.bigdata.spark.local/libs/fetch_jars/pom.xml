<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">

    <!-- We need to define spark-parent as the parent pom because otherwise 
		many library versions (e.g. derby) do not match exactly those that come with 
		the binary Spark build -->
    <parent>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-parent_2.12</artifactId>
        <version>3.5.3</version>
    </parent>

    <modelVersion>4.0.0</modelVersion>
    <groupId>org.knime.bigdata</groupId>
    <artifactId>spark-local-fetch-jars</artifactId>
    <version>3.5.3</version> <!-- Upload a copy of pyspark to our artifactory when changing this -->
    <packaging>pom</packaging>

    <name>Jar fetcher for local Spark</name>
    <description>Running the maven goal &quot;package&quot; on this maven project will fetch all jars required for local Spark and put them into the libs/ directory.</description>

    <properties>
        <avro.version>1.11.4</avro.version>
        <commons.compress.version>1.26.2</commons.compress.version>
        <gson.version>2.11.0</gson.version>
        <ivy.version>2.5.2</ivy.version>
        <parquet.version>1.15.1</parquet.version>
        <spark.version>3.5.3</spark.version>
        <zookeeper.version>3.8.4</zookeeper.version>
        <scalastyle.skip>true</scalastyle.skip>
        <checkstyle.skip>true</checkstyle.skip>
        <artifactory.login>${env.ARTIFACTORY_LOGIN}</artifactory.login>
        <artifactory.password>${env.ARTIFACTORY_PASSWORD}</artifactory.password>
        <spark.repo>https://${artifactory.login}:${artifactory.password}@artifactory.knime.com:443/artifactory/generic-downloads/pyspark</spark.repo>
    </properties>


    <!-- NOTE ABOUT DEALING WITH DEPENDENCIES:
	Many of the Spark dependencies (such as hadoop-client) are already part of KNIME.
	
	For *large* dependencies that are *already* part of KNIME we should avoid duplicating
	them in local Spark (for small dependencies it is usually not worth the hassle).
	
	*Large* in this context means: They contain a lot of classes (e.g. scala-library) or
	they have a lot of transitive dependencies (e.g. hadoop), or both.
	
	How to avoid duplication?
	
	Option (1)
	  Exclude them via build>plugins>plugin(maven-dependency-plugin)>executions>configuration (see at the bottom
	  of this file).
	  Pro: This leaves the maven dependency tree intact (for browsing), but prevents them from being copied to the libs/ directory.
	  Contra: A lot of work when dependencies have themselves have a lot of transitive dependencies, because you need to
	          exclude them manually too, then.
	  
	Option (2):
	  Exclude them via dependencies>dependency>exclusions.
	  Pro: Works well for dependencies that themselves have a lot of transitive dependencies.
	  Contra: Alters the maven dependency tree, which may be confusing.
	 -->
    <dependencies>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.12</artifactId>
            <version>${spark.version}</version>
            <exclusions>
                <!-- Netty 4.x is implicitly provided via the org.apache.hadoop.client OSGI bundle -->
                <exclusion>
                    <groupId>io.netty</groupId>
                    <artifactId>*</artifactId>
                </exclusion>
                <!-- provided via the org.apache.hadoop.client OSGI bundle -->
                <exclusion>
                    <groupId>org.apache.hadoop</groupId>
                    <artifactId>*</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>jdk.tools</groupId>
                    <artifactId>jdk.tools</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-avro_2.12</artifactId>
            <version>${spark.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>jdk.tools</groupId>
                    <artifactId>jdk.tools</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_2.12</artifactId>
            <version>${spark.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>jdk.tools</groupId>
                    <artifactId>jdk.tools</artifactId>
                </exclusion>
                <exclusion>
                  <groupId>org.codehaus.jackson</groupId>
                  <artifactId>jackson-mapper-asl</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive-thriftserver_2.12</artifactId>
            <version>${spark.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>jdk.tools</groupId>
                    <artifactId>jdk.tools</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-mllib_2.12</artifactId>
            <version>${spark.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>jdk.tools</groupId>
                    <artifactId>jdk.tools</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        
        <!-- BEGIN: version bumps to fix CVEs -->
        <dependency>
            <groupId>org.apache.avro</groupId>
            <artifactId>avro</artifactId>
            <version>${avro.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.zookeeper</groupId>
            <artifactId>zookeeper</artifactId>
            <version>${zookeeper.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>ch.qos.logback</groupId>
                    <artifactId>*</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>com.google.code.gson</groupId>
            <artifactId>gson</artifactId>
            <version>${gson.version}</version>
        </dependency>
        <dependency>
          <groupId>org.apache.commons</groupId>
          <artifactId>commons-compress</artifactId>
          <version>${commons.compress.version}</version>
        </dependency>
        <dependency>
          <groupId>org.apache.ivy</groupId>
          <artifactId>ivy</artifactId>
          <version>${ivy.version}</version>
        </dependency>
        <dependency>
          <groupId>org.apache.parquet</groupId>
          <artifactId>parquet-common</artifactId>
          <version>${parquet.version}</version>
        </dependency>
        <!-- END: version bumps to fix CVEs -->
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-dependency-plugin</artifactId>
                <version>3.1.2</version>
                <executions>
                    <execution>
                        <configuration>
                            <includeScope>runtime</includeScope>
                            <outputDirectory>../</outputDirectory>
                            <excludeGroupIds>org.apache.hadoop,org.scala-lang,org.scala-lang.modules,log4j,org.apache.logging.log4j</excludeGroupIds>
                        </configuration>
                        <phase>package</phase>
                        <goals>
                            <goal>copy-dependencies</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>io.github.download-maven-plugin</groupId>
                <artifactId>download-maven-plugin</artifactId>
                <version>2.0.0</version>
                <executions>
                    <execution>
                        <id>download-pyspark</id>
                        <phase>package</phase>
                        <goals>
                            <goal>wget</goal>
                        </goals>
                        <configuration>
                            <!-- To not hammer the servers of apache, we mirror pyspark to our artifactory, this is the download url where to find the upstream version.-->
                            <!-- https://archive.apache.org/dist/spark/spark-${spark.version}/pyspark-${spark.version}.tar.gz" -->
                            <url>${spark.repo}/pyspark-${spark.version}.tar.gz</url>
                            <unpack>false</unpack>
                            <outputDirectory>${project.build.directory}/tmp</outputDirectory>
                            <checkSignature>false</checkSignature>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-antrun-plugin</artifactId>
                <version>3.0.0</version>
                <executions>
                    <execution>
                        <id>get-pyspark</id>
                        <phase>package</phase>
                        <configuration>
                            <target name="download">
                                <echo message="unpack zips" />
                                <untar src="target/tmp/pyspark-${spark.version}.tar.gz" dest="../" compression="gzip">
                                    <patternset>
                                        <include name="**/lib/*.zip" />
                                    </patternset>
                                    <mapper type="flatten" />
                                </untar>
                            </target>
                        </configuration>
                        <goals>
                            <goal>run</goal>
                        </goals>
                    </execution>
                    <!-- Update bin.includes list of build.properties -->
                    <execution>
                        <id>generate-build-properties-bin-includes</id>
                        <phase>package</phase>
                        <configuration>
                            <target name="generate-build-properties-bin-includes">
                                <fileset dir=".." id="files.libs">
                                    <include name="*.jar"/>
                                    <include name="*.zip"/>
                                </fileset>
                                <pathconvert pathsep="," property="files.libs" refid="files.libs">
                                    <chainedmapper>
                                        <flattenmapper/>
                                        <globmapper from="*" to="libs/*"/>
                                    </chainedmapper>
                                </pathconvert>
                                <copy file="../../build.properties" tofile="build-tmp.properties" overwrite="true"/>
                                <copy file="build-tmp.properties" tofile="../../build.properties" overwrite="true">
                                    <filterchain>
                                        <replaceregex pattern="^bin.includes.*" byline="true"
                                            replace="bin.includes = META-INF/,knime.jar,plugin.xml,LICENSE.TXT,CHANGELOG.md,${files.libs}" />
                                    </filterchain>
                                </copy>
                                <delete file="build-tmp.properties"/>
                            </target>
                        </configuration>
                        <goals>
                            <goal>run</goal>
                        </goals>
                    </execution>
                    <!-- overwrite create-tmp-dir task from spark parent pom and use target/tmp -->
                    <execution>
                        <id>create-tmp-dir</id>
                        <configuration>
                          <target>
                            <mkdir dir="target/tmp" />
                          </target>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
            <!-- disable scala tests from spark parent pom -->
            <plugin>
                <groupId>org.scalatest</groupId>
                <artifactId>scalatest-maven-plugin</artifactId>
                <configuration>
                    <skip>true</skip>
                </configuration>
                <executions>
                    <execution>
                        <id>test</id>
                        <phase>none</phase>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>
