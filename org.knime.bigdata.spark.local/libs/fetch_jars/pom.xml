<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">

	<!-- We need to define spark-parent as the parent pom because otherwise 
		many library versions (e.g. derby) do not match exactly those that come with 
		the binary Spark build -->
	<parent>
		<groupId>org.apache.spark</groupId>
		<artifactId>spark-parent_2.12</artifactId>
		<version>3.0.2</version>
	</parent>

	<modelVersion>4.0.0</modelVersion>
	<groupId>org.knime.bigdata</groupId>
	<artifactId>spark-local-fetch-jars</artifactId>
	<version>3.0.2</version>
	<packaging>pom</packaging>

	<name>Jar fetcher for local Spark</name>
	<description>Running the maven goal &quot;package&quot; on this maven project will fetch all jars required for local Spark and put them into the libs/ directory.</description>

	<properties>
		<spark.version>3.0.2</spark.version>
		<scalastyle.skip>true</scalastyle.skip>
		<checkstyle.skip>true</checkstyle.skip>
	</properties>
	
	
	<!-- NOTE ABOUT DEALING WITH DEPENDENCIES:
	Many of the Spark dependencies (such as hadoop-client) are already part of KNIME.
	
	For *large* dependencies that are *already* part of KNIME we should avoid duplicating
	them in local Spark (for small dependencies it is usually not worth the hassle).
	
	*Large* in this context means: They contain a lot of classes (e.g. scala-library) or
	they have a lot of transitive dependencies (e.g. hadoop), or both.
	
	How to avoid duplication?
	
	Option (1)
	  Exclude them via build>plugins>plugin(maven-dependency-plugin)>executions>configuration (see at the bottom
	  of this file).
	  Pro: This leaves the maven dependency tree intact (for browsing), but prevents them from being copied to the libs/ directory.
	  Contra: A lot of work when dependencies have themselves have a lot of transitive dependencies, because you need to
	          exclude them manually too, then.
	  
	Option (2):
	  Exclude them via dependencies>dependency>exclusions.
	  Pro: Works well for dependencies that themselves have a lot of transitive dependencies.
	  Contra: Alters the maven dependency tree, which may be confusing.
	 -->
	<dependencies>
        <dependency>
            <groupId>org.apache.commons</groupId>
            <artifactId>commons-text</artifactId>
            <version>1.10.0</version>
        </dependency>
        <dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-sql_2.12</artifactId>
			<version>${spark.version}</version>
			<exclusions>
				<!-- Netty 3.x is implicitly provided via the org.apache.hadoop.client OSGI bundle -->
				<exclusion>
					<groupId>io.netty</groupId>
					<artifactId>netty</artifactId>
				</exclusion>
				<!-- provided via the org.apache.hadoop.client OSGI bundle -->
				<exclusion>
					<groupId>org.apache.hadoop</groupId>
					<artifactId>*</artifactId>
				</exclusion>
				<exclusion>
					<groupId>jdk.tools</groupId>
					<artifactId>jdk.tools</artifactId>
				</exclusion>
			</exclusions>
		</dependency>
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-avro_2.12</artifactId>
			<version>${spark.version}</version>
			<exclusions>
				<exclusion>
					<groupId>jdk.tools</groupId>
					<artifactId>jdk.tools</artifactId>
				</exclusion>
			</exclusions>
		</dependency>
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-hive_2.12</artifactId>
			<version>${spark.version}</version>
			<exclusions>
				<exclusion>
					<groupId>jdk.tools</groupId>
					<artifactId>jdk.tools</artifactId>
				</exclusion>
			</exclusions>
		</dependency>
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-hive-thriftserver_2.12</artifactId>
			<version>${spark.version}</version>
			<exclusions>
				<exclusion>
					<groupId>jdk.tools</groupId>
					<artifactId>jdk.tools</artifactId>
				</exclusion>
			</exclusions>
		</dependency>
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-mllib_2.12</artifactId>
			<version>${spark.version}</version>
			<exclusions>
				<exclusion>
					<groupId>jdk.tools</groupId>
					<artifactId>jdk.tools</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

	</dependencies>

	<build>
		<plugins>
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-dependency-plugin</artifactId>
				<version>3.1.2</version>
				<executions>
					<execution>
						<configuration>
							<includeScope>runtime</includeScope>
							<outputDirectory>../</outputDirectory>
							<excludeGroupIds>org.apache.hadoop,org.scala-lang,org.scala-lang.modules,log4j</excludeGroupIds>
						</configuration>
						<phase>package</phase>
						<goals>
							<goal>copy-dependencies</goal>
						</goals>
					</execution>
				</executions>
			</plugin>
			 <plugin>
				<groupId>org.apache.maven.plugins</groupId>  
				<artifactId>maven-antrun-plugin</artifactId>  
				<version>3.0.0</version>
				<executions>
					<execution>
						<id>get-pyspark</id>
						<phase>package</phase>  
						<configuration>  
							<target name="download"> 
								<get src="https://archive.apache.org/dist/spark/spark-${spark.version}/pyspark-${spark.version}.tar.gz" dest="target/tmp"/>
								<echo message="unpack zips" /> 
								<untar src="target/tmp/pyspark-${spark.version}.tar.gz" dest="../" compression="gzip">
									<patternset>
										<include name="**/lib/*.zip"/>
									</patternset>
									<mapper type="flatten"/>
								</untar>
							</target>
						</configuration>
						<goals>  
							<goal>run</goal>  
						</goals>  
					</execution>  
				</executions>  	
			 </plugin>
		</plugins>
	</build>
</project>
