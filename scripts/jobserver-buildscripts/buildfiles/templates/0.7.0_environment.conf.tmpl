# ############################################################################
# Spark Job Server configuration file. These settings are loaded when the
# job server starts. This file configures both the job-server as well as
# the settings of the Spark contexts it creates (properties in SparkConf).
#
# The latest documentation for the job server can be found here:
#   https://github.com/spark-jobserver/spark-jobserver/blob/master/README.md
#   https://github.com/spark-jobserver/spark-jobserver/tree/master/doc
#
# Documentation for configuring Spark contexts can be found on the Spark website
# (replace X.Y with your Spark version):
#  http://spark.apache.org/docs/1.X.Y/configuration.html
#
# If you are running Spark on YARN (yarn-client), then you might also find this
# useful:
#  http://spark.apache.org/docs/1.X.Y/running-on-yarn.html
#
# ############################################################################

spark {

  # #########################################################################
  # Job server settings
  # #########################################################################
  jobserver {
    # TCP port that the job server listens on for HTTP requests
    port = 8091

    # Directory where the job server stores uploaded jar files
    jar-store-rootdir = /tmp/%JSLINKNAME%/jars

    # Class to use to persist data such as jars, applications, jobs, etc.
    #   spark.jobserver.io.JobFileDAO uses the file system for persistence
    #   spark.jobserver.io.JobSqlDAO uses an SQL database for persistence
    #
    jobdao = spark.jobserver.io.JobSqlDAO
    sqldao {
      slick-driver = slick.driver.H2Driver
      jdbc-driver = org.h2.Driver
      rootdir = "/tmp/%JSLINKNAME%/sqldao/data"
      jdbc.url = "jdbc:h2:/tmp/%JSLINKNAME%/sqldao/data/h2-db;AUTO_SERVER=TRUE"

      dbcp {
        enabled = true
        maxactive = 20
        maxidle = 10
        initialsize = 10
      }
    }

    # Storage directory for files that are uploaded to the server
    # via POST/data commands
    datadao {
      rootdir = /tmp/%JSLINKNAME%/upload
    }

    # Number of jobs that can be run simultaneously per context. If not set,
    # defaults to number of cores on machine where Spark job server is running.
    max-jobs-per-context = 100

    # Timeouts for Spark context creation in seconds. In "yarn-client" mode, use
    # the first option, in all other modes, use the second one. If context
    # creation takes longer than the timeout, the jobserver closes the context.
    # Especially in yarn-client mode, context creation includes container
    # allocation, which can take a while.
    # yarn-context-creation-timeout = 40
    # context-creation-timeout = 60

    # If true, a separate JVM is forked for each Spark context.
    # KERBEROS NOTE: In a Kerberos-enabled environment, you should set this to true
    # (as well as shiro authentication, see later in this file).
    context-per-jvm = true
  }


  # #########################################################################
  # Spark settings (SparkConf of Spark context)
  # #########################################################################

  # Sets the "spark.master" property in the SparkConf. KNIME recommends
  # "yarn-client" for production use and "local[4]" for debugging purposes.
  master = "local[4]"           # Run Spark locally with 4 worker threads
  # master = "yarn-client"      # Connect to a YARN cluster in client mode. The
                                # cluster location will be found based on the
                                # HADOOP_CONF_DIR environment variable.
  # master = "spark://HOST:PORT"  # Connect to a Spark standalone cluster master.

  # Default settings for Spark contexts. These settings can be overridden on a
  # per-context basis. Please consult the Spark documentation for more details
  # on available settings.
  context-settings {

    # Required setting, that sets "spark.cores.max" in the SparkConf. This sets
    # the maximum amount of CPU cores to request for the Spark on the cluster
    # (not from each machine).
    #
    # IMPORTANT: Note that although required by job-server, this setting only
    # has an effect in Standalone and Mesos clusters.
    num-cpu-cores = 2

    # Required setting, that sets "spark.executor.memory" in the SparkConf. Sets
    # the amount of memory to use per Spark executor process, in the same format
    # as JVM memory strings (e.g. 512m, 2g).
    #
    # Note: In "yarn-client" mode, Spark will request this amount of memory per YARN
    # container, plus some additional overhead.
    memory-per-node = 1G

    # Useful settings for yarn-client mode. Please consult the Spark-on-YARN
    # documentation.
    # spark.executor.instances = 10
    # spark.executor.cores = 8

    # Useful settings to tune parallelism and Spark's memory management. Please
    # consult the Spark documentation.
    # spark.default.parallelism = 160
    # spark.storage.memoryFraction = 0.6

    # Useful setting to make Spark executors log garbage collection statistics,
    # that can be used to debug memory problems.
    # spark.executor.extraJavaOptions = "-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps"

    # If you wish to pass any settings directly to the SparkConf as-is, add them
    # here in passthrough, such as hadoop connection settings that don't use
    # the "spark." prefix.
    passthrough {
      # es.nodes = "192.1.1.1"
    }

    # uris of jars to be loaded into the classpath for this context. Uris is a string list, or a string separated by commas ','
    # dependent-jar-uris = ["file:///some/path/present/in/each/mesos/slave/somepackage.jar"]
  }

  # Predefined Spark contexts with individual names. You can put settings here
  # as documented in the default context settings section above.
  # contexts {
  #   my-low-latency-context {
  #     num-cpu-cores = 1
  #     memory-per-node = 512m
  #     # put optional settings here
  #   }
  #   # define additional contexts here
  # }

  # The Spark installation directory on machines running Spark executors. Unless
  # Spark is installed in a different location than on the job-server machine
  # (which executes the Spark driver), you can leave this commented out.
  # home = "/home/spark/spark"
}

spray.can.server {
  # parsing {
  #   max-content-length = 200m
  # }
  request-chunk-aggregation-limit = 200m
}

akka {
  remote.netty.tcp {
    # This controls the maximum message size, including job results, that can be sent
    maximum-frame-size = 100 MiB
  }
}

# Jobserver provides authentication via shiro. To configure authentication you need to write
# a shiro.ini file and reference it below. See the provided shiro.ini.xxx.template or the official Spark
# jobserver website for examples.
# KERBEROS NOTE: In a Kerberos-enabled environment, you should always use shiro authentication and use-as-proxy-user
#shiro {
   # Uncomment these lines and write a shiro.ini to have jobserver authenticate the calls to its REST interface
   # authentication = on
   # config.path = "shiro.ini"

   # Set this if you want the Spark contexts created by jobserver to always impersonate the (authenticated) user
   # that created the context via REST.
   # use-as-proxy-user = on
# }
