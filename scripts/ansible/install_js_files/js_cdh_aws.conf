# ############################################################################
# Spark Job Server configuration file. These settings are loaded when the
# job server starts. This file configures both the job-server as well as
# the settings of the Spark contexts it creates (properties in SparkConf).
#
# The latest documentation for the job server can be found here:
#   https://github.com/spark-jobserver/spark-jobserver/blob/master/README.md
#   https://github.com/spark-jobserver/spark-jobserver/tree/master/doc
#
# Documentation for configuring Spark contexts can be found on the Spark website
# (replace X.Y with your Spark version):
#  http://spark.apache.org/docs/1.X.Y/configuration.html
#
# If you are running Spark on YARN (yarn-client), then you might also find this
# useful:
#  http://spark.apache.org/docs/1.X.Y/running-on-yarn.html
#
# ############################################################################

spark {

  # #########################################################################
  # Job server settings
  # #########################################################################
  jobserver {
    # TCP port that the job server listens on for HTTP requests
    port = 8090

    # Directory where the job server stores uploaded jar files
    jar-store-rootdir = /tmp/spark-jobserver/jars

    # Class to use to persist data such as jars, applications, jobs, etc.
    #   spark.jobserver.io.JobFileDAO uses the file system for persistence
    #   spark.jobserver.io.JobSqlDAO uses an SQL database for persistence
    #
    jobdao = spark.jobserver.io.JobSqlDAO 
    sqldao {
      rootdir = "/tmp/spark-jobserver/sqldao/data"
    }

    # Storage directory for files that are uploaded to the server
    # via POST/data commands
    datadao {
      rootdir = /tmp/spark-jobserver/upload
    }

    # Number of jobs that can be run simultaneously per context. If not set,
    # defaults to number of cores on machine where Spark job server is running.
    max-jobs-per-context = 200

    # Timeouts for Spark context creation in seconds. In "yarn-client" mode, use
    # the first option, in all other modes, use the second one. If context
    # creation takes longer than the timeout, the jobserver closes the context.
    # Especially in yarn-client mode, context creation includes container
    # allocation, which can take a while.
    # yarn-context-creation-timeout = 40
    # context-creation-timeout = 60

    # If true, a separate JVM is forked for each Spark context
    context-per-jvm = false
  }


  # #########################################################################
  # Spark settings (SparkConf of Spark context)
  # #########################################################################

  # Sets the "spark.master" property in the SparkConf. KNIME recommends
  # "yarn-client" for production use and "local[4]" for debugging purposes.
  # master = "local[2]"           # Run Spark locally with 4 worker threads
  master = "yarn-client"      # Connect to a YARN cluster in client mode. The
                                # cluster location will be found based on the
                                # HADOOP_CONF_DIR environment variable.
  # master = "spark://HOST:PORT"  # Connect to a Spark standalone cluster master.

  # Default settings for Spark contexts. These settings can be overridden on a
  # per-context basis. Please consult the Spark documentation for more details
  # on available settings.
  context-settings {

    # Required setting, that sets "spark.cores.max" in the SparkConf. This sets
    # the maximum amount of CPU cores to request for the Spark on the cluster
    # (not from each machine).
    #
    # IMPORTANT: Note that although required by job-server, this setting only
    # has an effect in Standalone and Mesos clusters.
    num-cpu-cores = 2

    # Required setting, that sets "spark.executor.memory" in the SparkConf. Sets
    # the amount of memory to use per Spark executor process, in the same format
    # as JVM memory strings (e.g. 512m, 2g).
    #
    # Note: In "yarn-client" mode, Spark will request this amount of memory per YARN
    # container, plus some additional overhead.
    memory-per-node = 2000M

    # Useful settings for yarn-client mode. Please consult the Spark-on-YARN
    # documentation.
    # spark.executor.instances = 2
    # spark.executor.cores = 4

    # Useful settings to tune parallelism and Spark's memory management. Please
    # consult the Spark documentation.
    # spark.default.parallelism = 160
    # spark.storage.memoryFraction = 0.6

    # Useful setting to make Spark executors log garbage collection statistics,
    # that can be used to debug memory problems.
    # spark.executor.extraJavaOptions = "-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps"

    # If you wish to pass any settings directly to the SparkConf as-is, add them
    # here in passthrough, such as hadoop connection settings that don't use
    # the "spark." prefix.
    passthrough {
      # es.nodes = "192.1.1.1"
    }

    # uris of jars to be loaded into the classpath for this context. Uris is a string list, or a string separated by commas ','
    # dependent-jar-uris = ["file:///some/path/present/in/each/mesos/slave/somepackage.jar"]
  }

  # Predefined Spark contexts with individual names. You can put settings here
  # as documented in the default context settings section above.
  # contexts {
  #   my-low-latency-context {
  #     num-cpu-cores = 1
  #     memory-per-node = 512m
  #     # put optional settings here
  #   }
  #   # define additional contexts here
  # }

  # The Spark installation directory on machines running Spark executors. Unless
  # Spark is installed in a different location than on the job-server machine
  # (which executes the Spark driver), you can leave this commented out.
  # home = "/home/spark/spark"
}
