# create user homedirs in secured clusters
- hosts: hadoop-hosts:&master-*:&secure-hosts
  tasks:
    - name: Copy hdfs keytab
      copy: src={{keytab_dir}}/{{hdfs_keytab}} dest=/root/hdfs.keytab owner=root mode=u=rw,go=

    - name: Get hdfs TGT
      shell: kdestroy ; kinit -kt /root/hdfs.keytab hdfs

    - name: create HDFS homedirs for users
      shell: for u in test tobias sascha knime bjoern ; do hdfs dfs -mkdir -p /user/$u ; hdfs dfs -chown -R $u /user/$u ; done

# create user homedirs in unsecured clusters
- hosts: hadoop-hosts:&master-*:!secure-hosts
  tasks:
    - name: create HDFS homedirs for users
      become: yes
      become_user: hdfs
      shell: for u in test tobias sascha knime bjoern ; do hdfs dfs -mkdir -p /user/$u ; hdfs dfs -chown -R $u /user/$u ; done


# - hosts: master-*
#   vars:
#     ansible_become_user: hdfs
#   tasks:
#     - name: upload cdr test data via SSH
#       copy: src=normalize_files/CDR dest=/tmp/ owner=hdfs
#     - name: upload iris test data via SSH
#       copy: src=normalize_files/iris dest=/tmp/ owner=hdfs
#     - shell: hdfs dfs -rm -r -f -skipTrash /demo/data
#     - shell: hdfs dfs -mkdir -p /demo/data
#     - shell: hdfs dfs -put -p /tmp/CDR /demo/data/
#     - shell: hdfs dfs -put -p /tmp/iris /demo/data/
#     - shell: hdfs dfs -chmod -R ugo+rx /demo
#     - shell: beeline -u "jdbc:hive2://localhost:10000/" -u hive -e "query..."

# - hosts: master-hdp-*
#   tasks:
#     - name: install httpfs via yum
#       yum: name=hadoop-httpfs state=latest
#
#     - name: Detect HDP version
#       shell: hdp-select status hadoop-httpfs | grep -o '[.0-9-]\+$'
#       register: hdpversion
#
#     - name: hdp-select httpfs
#       shell: hdp-select set hadoop-httpfs {{hdpversion.stdout}}
#
#     - name: copy httpfs.sh script
#       copy: src=normalize_files/httpfs.sh dest=/usr/hdp/{{hdpversion.stdout}}/hadoop-httpfs/sbin/httpfs.sh force=yes backup=yes
#
#     - name: link tomcat config
#       file: src=/etc/hadoop-httpfs/tomcat-deployment/conf dest=/usr/hdp/{{hdpversion.stdout}}/hadoop-httpfs/conf owner=root group=root state=link
#
#     - name: link hadoop-libexec
#       file: src=../hadoop/libexec dest=/usr/hdp/{{hdpversion.stdout}}/hadoop-httpfs/libexec owner=root group=root state=link
#
#     - name: copy httpfs-env.sh
#       copy: src=normalize_files/httpfs-env.sh dest=/etc/hadoop-httpfs/conf/httpfs-env.sh force=yes backup=yes
#
#     - stat: path=/usr/hdp/{{hdpversion.stdout}}/etc/rc.d/init.d/hadoop-httpfs
#       register: initd_old_location
#
#     - name: add httpfs init.d script
#       file: src=/usr/hdp/{{hdpversion.stdout}}/etc/rc.d/init.d/hadoop-httpfs dest=/etc/init.d/hadoop-httpfs owner=root group=root state=link
#       when: initd_old_location.stat.exists == True
#
#     - name: add httpfs init.d script
#       file: src=/usr/hdp/{{hdpversion.stdout}}/hadoop-httpfs/etc/rc.d/init.d/hadoop-httpfs dest=/etc/init.d/hadoop-httpfs owner=root group=root state=link
#       when: initd_old_location.stat.exists == False
#
#     - name: create logdir
#       file: path=/var/log/hadoop-httpfs state=directory owner=httpfs  group=httpfs
#     - service: name=hadoop-httpfs enabled=yes
#     - service: name=hadoop-httpfs state=started

    # ATTENTION: this breaks the HDP sandbox VMs!
    # - name: install jq via yum
    #   yum: name=jq state=latest
    # - name: Detect Ambari cluster name
    #   shell: curl -s -u admin:admin "http://localhost:8080/api/v1/clusters" | jq -r ".items[0].Clusters.cluster_name"
    #   register: ambcluster
    # - name: allow httpfs to impersonate others (set via ambari)
    #   shell: /var/lib/ambari-server/resources/scripts/configs.sh set localhost {{ambcluster.stdout}} core-site 'hadoop.proxyuser.httpfs.groups' '*' ; /var/lib/ambari-server/resources/scripts/configs.sh set localhost {{ambcluster.stdout}} core-site 'hadoop.proxyuser.httpfs.hosts' '*'
    # - name: allow httpfs to impersonate others (set directly into core-site.xml)
    #   blockinfile:
    #     dest: /etc/hadoop/conf/core-site.xml
    #     marker: "<!-- {mark} ANSIBLE MANAGED BLOCK -->"
    #     insertbefore: "</configuration>"
    #     content: |
    #       <property>
    #         <name>hadoop.proxyuser.httpfs.groups</name>
    #         <value>*</value>
    #       </property>
    #       <property>
    #         <name>hadoop.proxyuser.httpfs.hosts</name>
    #         <value>*</value>
    #       </property>
