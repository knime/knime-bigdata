<?xml version="1.0" encoding="UTF-8"?>
<knimeNode xmlns="http://knime.org/node/v2.12" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
    xsi:schemaLocation="http://knime.org/node/v2.12 http://knime.org/node/v2.12.xsd"
	type="Manipulator" icon="../icon.png">
	<name>Spark Data Frame Java Snippet</name>
	<shortDescription>Execute arbitrary java code in Spark.</shortDescription>
    <fullDescription>
        <intro>
            <p>This node allows you to execute arbitrary java code to manipulate or create Spark DataFrames.
             Simply enter the java code in the text area.</p>
            <p>Note, that this node also supports flow variables as input to your Spark job. 
            To use a flow variable simply double click on the variable in the "Flow Variable List".</p>
            <p>It is also possible to use external java libraries. In order to 
            include such external jar or zip files, add their location in the 
            "Additional Libraries" tab using the control buttons.
            For details see the "Additional Libraries" tab description below.
            <br/>
            <b>The used libraries need to be present on your cluster and added to the class path of your Spark job server.
            They are not automatically uploaded!</b>
            <br/>
            </p>
            
            <p>You can define reusable templates with the "Create templates..." 
            button. Templates are stored in the users workspace by default and can be accessed via the "Templates"
            tab. For details see the "Templates" tab description below.</p>
        </intro>
        <tab name="Java Snippet">
            <option name="Flow Variable List">
              The list contains the flow variables that are currently available at
              the node input. Double clicking any of the entries will insert the
              respective identifier at the current cursor position (replacing the
              selection, if any).
            </option>
            <option name="Snippet text area">
              <p>Enter your java code here.</p>

              <p>
	          The <i>SparkSession</i> can be accessed via the method input parameter <i>spark</i>.
              The input <i>Dataset&lt;Row&gt;</i> can be accessed via the method input parameter <i>dataFrame1</i> and <i>dataFrame2</i>
              whereas <i>dataFrame2</i> is <i>null</i> if the input port is not connected.</p>

              <p><b>Flow variables:</b><br/>
              You can access input flow variables by defining them in the <b>Input</b> table.
              To define a flow variable simply double click on the variable in the "Flow Variable list".</p>

              <p>You can hit <i>ctrl+space</i> to get an auto completion box with all
              available classes, methods and fields. When you select a class and hit
              <i>enter</i> a import statement will be generated if missing.</p>          

              <p>Note, that the snippet allows to define custom global variables and
              custom imports. To view the hidden editor parts simply click on the plus symbols in the editor.</p>
            </option>
            <option name="Input">
              Define system input fields for the snippet text area. Every field will 
              be populated with the data of the defined input during execution. 
            </option>
       </tab>
       <tab name="Additional Libraries">
           <description>
               Allows you to add additional jar files to the java snippet class path.
               <br/>
               <b>The used libraries need to be present on your cluster and added to the class path of your Spark job server.
            They are not automatically uploaded!</b>
           </description>
           <option name="Add File(s)">Allows you to include local jar files.</option>
           <option name="Add KNIME URL...">Allows you to add workflow relative jar files.</option>
       </tab>
       <tab name="Templates">
           <description>Provides predefined templates and allows you to define new reusable templates by saving the
           current snippet state.</description>
           <option name="Category">Groups templates into different categories.</option>
           <option name="Apply">Overwrites the current node settings with the template settings.</option>
           <option name="Java Snippet">Preview of the template code.</option>
           <option name="Additional Libraries">Preview of the additional jars.</option>
       </tab>
    </fullDescription>

	<ports>
        <inPort index="0" name="First Spark Data">First input Spark DataFrame.</inPort>
        <inPort index="1" optional="true" name="Second Spark Data">Optional second input Spark DataFrame.</inPort>
        <outPort index="0" name="Result Spark Data">Result Spark DataFrame.</outPort>
	</ports>
</knimeNode>
