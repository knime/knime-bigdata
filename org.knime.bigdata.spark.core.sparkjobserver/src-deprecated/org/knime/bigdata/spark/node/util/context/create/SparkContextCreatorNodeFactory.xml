<?xml version="1.0" encoding="UTF-8"?>
<knimeNode icon="icon.png" type="Source" xmlns="http://knime.org/node/v2.12" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
    xsi:schemaLocation="http://knime.org/node/v2.12 http://knime.org/node/v2.12.xsd" deprecated="true">
    <name>Create Spark Context (Jobserver)</name>
    
    <shortDescription>Creates a new Spark context via Spark Jobserver.</shortDescription>
    
    <fullDescription>
        <intro>
            Creates a new Spark context via Spark Jobserver. 

            <p>Support for Spark Jobserver is deprecated and the
            <a href="https://kni.me/n/KTzMyfLj7V8lGhrz">Create Spark Context (Livy)</a> node should be used instead.</p>

            <p>The default settings of the node are taken from the Spark preference page which
            you can open via <i>File->Preferences->KNIME->Big Data->Spark</i>.</p>
        </intro>
        <tab name="Context Settings">
        <option name="Spark version">
        	The Spark version used by Spark Jobserver.
        </option>
        <option name="Context name">The unique name of the context.</option>
        <option name="Destroy Spark context on dispose">If selected, the Spark context will be destroyed when the workflow or
        KNIME is closed. This way, resources on the cluster are released, but all data cached inside the Spark context
        are lost, unless they have been saved to persistent storage such as HDFS.
        </option>
        <option name="Delete Spark DataFrames/RDDs on dispose">If selected, KNIME deletes the created RDDs/DataFrames when 
        the workflow or KNIME is closed. This way, resources on the cluster are released, but all data that the current workflow
        holds in the Spark context are lost, unless they have been saved to persistent storage such as HDFS.
        </option>
        <option name="Spark job log level">The log level to use for Spark jobs within the Spark runtime.</option>
        <option name="Override Spark settings">Select this option to set custom Spark settings.</option>
        <option name="Custom Spark settings">Custom Spark settings to add to or overwrite the default settings defined by Spark Jobserver. 
        See <a href="https://spark.apache.org/docs/latest/configuration.html">Spark documentation</a> for more information.
        </option>
	<option name="Hide warning about an existing Spark context">Enable this option to suppress a warning message
		shown when the Spark context to be created by this node already
		exists.
		</option>
        </tab>
        <tab name="Connection Settings">
        <option name="Jobserver URL">The URL of the Spark Jobserver including protocol and port e.g. http://localhost:8090.</option>
	<option name="Authentication">
		Select
		<ul>
			<li>
				<i>None</i>, if Spark jobserver does not require any credentials.
			</li>
			<li>
				<i>Username &amp; password</i>
				and enter the respective credentials, which will be saved with the
				workflow.
			</li>
			<li>
				<i>Credentials</i>
				and select from the available credentials.
			</li>
		</ul>
	</option>
	<option name="Jobserver response timeout (seconds)">Time to wait for a response when making a request
		to Spark Jobserver (0 is infinite).
	</option>
	<option name="Spark job check frequency (seconds)">The frequency with which KNIME polls the status of
		a job.
	</option>        
    </tab>
    </fullDescription>
    
    <ports>
        <outPort index="0" name="Spark context">Spark context.</outPort>
    </ports>    
</knimeNode>
